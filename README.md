# Databricks Basic

## Sesión 1:  Ingeniería de Datos y Pipeline ETL con Delta Lake

*Objetivo*: Construir pipelines ETL, optimizar procesos y entender la configuración de 
clusters para cargas de trabajo de ingeniería (2h). 

• Recapitulación breve y presentación de ETL. 

• Ingesta de datos desde distintos orígenes (CSV, Parquet, JSON, bases de datos con JDBC) 

• Introducción a Delta Lake: qué es, ventajas y uso en Databricks 

• Fundamentos de Delta Lake: ACID, time travel y optimización de datos. 

• Construcción de pipelines ETL en Databricks: mejores prácticas y ejemplos prácticos usando usando notebooks y Delta Lake. 

• Ejercicio práctico: pipeline básico de ETL con PySpark y Delta Lake 

## Sesión 2:  Manejo de Permisos, Workflows y Jobs en entornos productivos 

*Objetivo*:  Aprender a automatizar tareas, controlar el acceso y ejecutar flujos de trabajo programados (2h). 

• Introducción a los Jobs en Databricks: configuración, scheduling y monitoreo de pipelines. 

• Integración con herramientas de orquestación externas (como Airflow) y CI/CD (repositorios Git) para notebooks. 

• Permisos sobre clusters, notebooks, bases de datos y tablas 

• Uso colaborativo: comentarios, versionado, notebooks compartidos 

• Ejercicio práctico, discusión de mejores prácticas y sesión de preguntas. 


## Sesión 3:  Machine Learning en Producción con MLflow y Databrick

*Objetivo*: Objetivo: Mostrar el potencial de ML en Databricks y qué herramientas usar para lograrlo (2h).  

• Introducción a MLflow: tracking de experimentos, modelos y métricas 

• Entrenamiento de un modelo básico en un notebook 

• Registro y despliegue del modelo en MLflow 

• Ejercicio práctico: experimento de ML con MLflow 

• Cierre final del curso: dudas, preguntas y aclaraciones 