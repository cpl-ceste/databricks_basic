{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efb84b5f-2ddc-47ab-a8cc-59c78c783285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks: ETLs y Delta Lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdbd4652-db73-44e5-89cc-6d9253da6a92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ¬øQu√© es un ETL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d1e1c9a-3b95-4383-8054-9f19e73de922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Un ETL (Extract, Transform, Load) es un proceso fundamental en la ingenier√≠a de datos. Consiste en extraer datos de diferentes fuentes, transformarlos para adecuarlos a las necesidades del negocio y cargarlos en un sistema de almacenamiento o an√°lisis, como un Data Lake o un Data Warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2112e823-b190-4b5e-9d5b-0f80edbd3641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8b60ccf-9427-430f-a6b9-47ef21d98d34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### dbutils\n",
    "`dbutils` es una colecci√≥n de utilidades proporcionadas por Databricks para interactuar con el entorno, gestionar archivos, par√°metros y flujos de trabajo. Es muy √∫til para la gesti√≥n de datos y automatizaci√≥n de tareas dentro de notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf1ffea-9c2b-41f0-b5b6-e15b1fdbe997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83405a7e-0fa7-4104-a8a2-3ccc20d4d640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Dentro del cat√°logo de `dbutils` existen subsecciones como:\n",
    " - dbutils.fs: Para interactuar con el sistema de archivos\n",
    " - dbutils.secrets: Para usar secretos\n",
    " - dbutils.notebook: Para poder interactuar con otros notebooks\n",
    " - dbutils.widgets: Para poder parametrizar los notebooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "767c754b-2cf6-4e1e-a9df-1cc3dc31dca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**IMPORTANTE**: Antes de seguir subir a Databricks los ficheros del repositorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "697f2c65-fb94-416d-b72a-4ab21f511ed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Pista.** Para hacerlo sigue estos pasos. No obstante, si alg√∫n alumno prefiere guardar los ficheros en otro lugar puede hacerlo sin problema.\n",
    "1. Ir a _Catalog_\n",
    "2. Entrar en el Catalogo \"workspace\"\n",
    "3. Crear un _schema_ con nombre \"ceste\"\n",
    "4. Crear un _volumne_ con nombre \"archivos\"\n",
    "5. Pulsar en a√±adir y subir los archivos \"ventas.csv\", \"ventas.json\", \"ventas.parquet\"\n",
    "6. Comprobar si se han subido bien ejecutando la celda de abajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4cfe470-f801-44d8-afc2-9867098e848f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"dbfs:/Volumes/workspace/ceste/archivos/\"\n",
    "dbutils.fs.ls(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3db3871-5f2c-4ac9-8bbb-3db76f54fa87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingesta desde Distintos Or√≠genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbb408d9-dcc8-467a-a410-ab737706661f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "En los entornos de Big Data, los datos pueden venir en m√∫ltiples formatos: CSV, Parquet, JSON, entre otros. Cada formato tiene ventajas y desventajas en cuanto a compresi√≥n, velocidad de lectura/escritura y compatibilidad. Parquet, por ejemplo, es columnar y eficiente para grandes vol√∫menes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d73bcc61-ceca-4d4b-b049-45b89084df61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Leer desde CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da8f6013-6bde-4d4c-8baa-37cbbccb392a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Leer datos desde un archivo CSV es una de las formas m√°s comunes de ingesta en proyectos de datos. El formato CSV (Comma Separated Values) es ampliamente utilizado por su simplicidad y compatibilidad con la mayor√≠a de las herramientas. Sin embargo, es importante tener en cuenta que no es el formato m√°s eficiente para grandes vol√∫menes de datos, ya que no soporta tipos de datos complejos ni compresi√≥n nativa. Por eso, en entornos de Big Data, a menudo se prefiere convertir los datos a formatos como Parquet o Delta una vez cargados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "764443d7-9a67-45a9-9f0e-4a02c1b834d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv = spark.read.csv(\n",
    "    path=base_path+\"ventas.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d85a08b5-0144-4473-914e-01b1848f4480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_csv.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62d48cb5-6bd3-431b-bbb7-b850e31ef9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Leer desde parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "981cc141-81ac-4f40-bb1f-43ce51f41d25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "El formato Parquet es un est√°ndar de almacenamiento columnar ampliamente utilizado en Big Data. Su principal fortaleza es la eficiencia tanto en almacenamiento como en velocidad de lectura, especialmente cuando se trabaja con grandes vol√∫menes de datos y consultas sobre columnas espec√≠ficas. Parquet permite compresi√≥n y soporta tipos de datos complejos, lo que lo hace ideal para an√°lisis y procesamiento distribuido. Por ello, es habitual convertir datos de formatos como CSV a Parquet para optimizar el rendimiento y reducir costes en proyectos de an√°lisis de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e27776-310a-4f14-9785-83c9418b5d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(base_path+\"ventas.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "499ff56a-e6ad-4bec-a4e7-3afa2789f67e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfdc0227-aefa-4788-b566-d9e320b3ed95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Leer desde Json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93f12daf-6dd8-47ba-9ffa-0cc526e37034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "El formato JSON (JavaScript Object Notation) es ampliamente utilizado para el intercambio de datos debido a su flexibilidad y legibilidad. Permite almacenar estructuras de datos complejas, como listas y diccionarios anidados. Sin embargo, en comparaci√≥n con Parquet, JSON no es tan eficiente en almacenamiento ni en velocidad de procesamiento para grandes vol√∫menes de datos. Es √∫til cuando se requiere flexibilidad en la estructura de los datos o cuando los datos provienen de APIs y sistemas web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2090c386-4402-4ff4-be90-f95eda06cd54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json = spark.read.json(base_path+\"ventas.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cef43e1-1a43-4cd7-b718-0496bad5e2ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9026c06-aa32-40e6-804e-cae5f43f2e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Leer desde BD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e644065-fa5d-44c8-b346-aab926e018fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "La lectura de datos desde bases de datos (BD) es fundamental cuando los datos se encuentran almacenados en sistemas transaccionales o relacionales, como MySQL, SQL Server, PostgreSQL, entre otros. En entornos de Big Data, es com√∫n extraer datos de estas fuentes para integrarlos en un Data Lake o procesarlos con Spark. La conexi√≥n suele realizarse mediante JDBC, permitiendo ejecutar consultas SQL y cargar los resultados como DataFrames. Es importante considerar la seguridad, el rendimiento y la gesti√≥n de credenciales al conectar con bases de datos externas.\n",
    "\n",
    "*La siguiente celda no va a hacer nada porque para funcionar necesitar√≠amos una de BD activa*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d82d8e13-8128-4d4f-8f67-29c09e1fbd01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:mysql://<host>:<port>/<database>\"\n",
    "properties = {\"user\": \"user\", \"password\": \"pwd\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b80debf-9923-48c1-9b70-cabdf3fc407b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Fuente   | ‚úîÔ∏è Ventajas                                                                 | ‚ùå Desventajas                                                                | Uso Com√∫n                                               |\n",
    "----------|-------------------------------------------------------------------------|----------------------------------------------------------------------------|---------------------------------------------------------|\n",
    " CSV      | Simple y ampliamente compatible. F√°cil de editar manualmente.            | No soporta tipos de datos complejos ni compresi√≥n nativa. Menos eficiente para grandes vol√∫menes. | Ingesta inicial de datos peque√±os o de fuentes externas.|\n",
    " Parquet  | Almacenamiento columnar eficiente, compresi√≥n nativa, r√°pido para consultas en columnas espec√≠ficas. | No es legible por humanos. Requiere herramientas espec√≠ficas para edici√≥n.  | Procesamiento de Big Data, an√°lisis y almacenamiento optimizado.|\n",
    " JSON     | Flexible para estructuras complejas (anidadas). Legible y compatible con APIs web. | Menos eficiente en almacenamiento y procesamiento para grandes vol√∫menes.   | Intercambio de datos con sistemas web o cuando se necesita flexibilidad en la estructura.|\n",
    " BD       | Acceso directo a datos actualizados, integraci√≥n con sistemas empresariales, soporte para consultas SQL. | Requiere configuraci√≥n de conexi√≥n, puede tener limitaciones de rendimiento y permisos. | An√°lisis de datos operacionales, integraci√≥n de datos de negocio.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4f2d65d-00ec-498a-a111-472510b4dd89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Ejercicio Guiado: Cargar el Dataset \"ventas.csv\" con Esquema\n",
    "\n",
    "En este ejercicio, aprender√°s a cargar el dataset \"ventas.csv\" utilizando un esquema definido manualmente en lugar de inferirlo autom√°ticamente. Esto es √∫til para asegurar la consistencia de los tipos de datos y mejorar el rendimiento.\n",
    "\n",
    "**Pasos a seguir:**\n",
    "\n",
    "1. **Define el esquema manualmente:** Utiliza `StructType` y `StructField` para especificar los tipos de datos de cada columna. Bas√°ndote en el contenido del archivo, el esquema podr√≠a incluir campos como `id` (StringType), `fecha` (StringType), `producto` (StringType), `cantidad` (IntegerType) y `precio` (DoubleType).\n",
    "\n",
    "2. **Lee el archivo CSV con el esquema:** Usa `spark.read.csv()` con los par√°metros `header=True`, `schema=tu_esquema`, `sep=\",\"` y el path `base_path+\"ventas.csv\"`.\n",
    "\n",
    "3. **Verifica el esquema y muestra los datos:** Imprime el esquema con `printSchema()` y muestra las primeras filas con `display()` o `show()`.\n",
    "\n",
    "**C√≥digo de ejemplo (completa las partes faltantes):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f56535e-a555-47a9-824c-1eca535f33a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "### Tu codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6330c05-f3dd-4994-86f2-8c7dd5436160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Recordatorio: Leer con/sin esquema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f145f50-87f4-4f61-8333-b5c6bb7e0ae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| M√©todo de Lectura         | ‚úîÔ∏è Ventajas                                                                 | ‚ùå Desventajas                                                            | Uso Recomendado                          |\n",
    "|--------------------------|--------------------------------------------------------------------------|------------------------------------------------------------------------|------------------------------------------|\n",
    "| **Sin esquema (inferSchema=True)** | - R√°pido para exploraci√≥n inicial<br>- No requiere conocer los tipos de datos previamente | - Puede inferir tipos incorrectos si los datos son inconsistentes<br>- M√°s lento en archivos grandes<br>- Menos robusto en producci√≥n | Exploraci√≥n, pruebas r√°pidas, datos peque√±os o desconocidos |\n",
    "| **Con esquema definido** | - Tipos de datos consistentes y controlados<br>- Mejor rendimiento en la carga<br>- Evita errores por inferencia incorrecta | - Requiere conocer la estructura de los datos<br>- M√°s trabajo inicial | Procesos productivos, datos cr√≠ticos, ETLs, grandes vol√∫menes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17609ccb-8ded-4efa-b6a1-8e42ae89fb7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef950c5c-16c7-43da-a2c7-882a33778a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ¬øQu√© es Delta Lake?\n",
    "Delta Lake es una capa de almacenamiento open source que se integra con Apache Spark y a√±ade capacidades ACID (Atomicidad, Consistencia, Aislamiento, Durabilidad) a los Data Lakes. Permite versionado, time travel, y operaciones transaccionales sobre los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "986a7c81-3667-40e4-988b-c855438c9dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ¬øPor qu√© es tan valioso el formato de Delta Table?\n",
    "Guardar tablas en formato Delta permite aprovechar las ventajas de transacciones ACID, manejo de versiones, y optimizaci√≥n de consultas. Es ideal para entornos donde los datos cambian frecuentemente y se requiere trazabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fe29e46-ed26-45c4-9b4d-95c54f3010cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Una Delta Table permite realizar operaciones ACID, mantener el hist√≥rico con time travel, gestionar versiones, optimizar el almacenamiento, y escalar en entornos de producci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05acd30a-9588-4859-ae93-2a5b31351f4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Guardar una tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c95a1e30-ec56-4ebf-83da-71268aab5d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Guardar una tabla en Databricks puede hacerse de dos formas principales:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "302cfef4-744d-4597-bb81-03c89d2ba0db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| M√©todo | Descripci√≥n | ‚úîÔ∏è Ventajas | Uso Recomendado |\n",
    "|--------|-------------|----------|-----------------|\n",
    "| **Por path** | Guarda los datos en una ruta espec√≠fica del sistema de archivos (ej: DBFS) usando formato Delta. | - Flexible y directo<br>- F√°cil de mover entre entornos<br>- Integraci√≥n con sistemas externos | Automatizaci√≥n, migraciones, acceso directo por ruta |\n",
    "| **Por cat√°logo del metastore** | Registra la tabla en el cat√°logo de Databricks para consultas SQL y control de acceso. | - Ideal para colaboraci√≥n multiusuario<br>- Control de permisos y versiones<br> - Auditor√≠a y seguridad avanzada<br>- F√°cil acceso mediante SQL | Entornos colaborativos, equipos de an√°lisis, integraci√≥n con BI |\n",
    "\n",
    "**Nota:** Ambas opciones aprovechan las ventajas del formato Delta: transacciones ACID, versionado y optimizaci√≥n de consultas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93f580ba-290a-4ea3-859d-7d144eb9de49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Por path\n",
    "df_csv.write.format(\"delta\").mode(\"overwrite\").save(base_path+\"delta\")\n",
    "\n",
    "# Por catalogo\n",
    "df_csv.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ceste.productos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb24fd8c-e562-4873-a4a1-e7f6a3f9ff7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Leer una tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d2fa14b-ccee-4aa6-9f4a-ab5ac9024743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Lectura de una tabla Delta desde Spark: Tambi√©n puedes leer una tabla Delta directamente desde Spark usando el API de DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c990b0d-09e7-477a-a64a-dbfd5eefca54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_delta = spark.read.format(\"delta\").load(base_path+\"delta\")\n",
    "df_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60fbf1a4-8b58-4ddb-8a86-f461173878f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Esto es √∫til cuando necesitas manipular los datos con Python, realizar transformaciones complejas, aplicar l√≥gica de negocio o integrarlo en pipelines de procesamiento.\n",
    "\n",
    "2. Consulta SQL sobre una tabla Delta: Puedes consultar una tabla Delta registrada en el cat√°logo usando SQL est√°ndar. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09949e50-5c80-4fe8-b964-1893f55c5d2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM ceste.productos;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5f25b95-c501-4586-9c7c-5bc5f84d9065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Esto te permite aprovechar toda la potencia del lenguaje SQL para filtrar, agrupar, unir y analizar los datos almacenados en formato Delta. Es especialmente √∫til para usuarios que prefieren trabajar con SQL o para integraciones con herramientas de BI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18226366-f549-4a3f-87c4-ea2bc2942dbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| M√©todo                |       ‚úîÔ∏è Ventajas        | Diferencias| Uso recomendado|\n",
    "|-----------------------|--------------------|------------|----------------|\n",
    "| **Spark DataFrame API** | - Procesamiento avanzado<br>- Integraci√≥n con ML y ETL<br>- Automatizaci√≥n y pipelines<br>- Flexibilidad en transformaciones                         | Permite l√≥gica compleja y manipulaci√≥n program√°tica de datos.  | Procesos autom√°ticos, machine learning, ETL, integraci√≥n con Python/Scala.|\n",
    "| **SQL**               | - F√°cil de usar y compartir<br>- Ideal para an√°lisis exploratorio<br>- Integraci√≥n con dashboards y BI<br>- Colaboraci√≥n multiusuario         | Sintaxis declarativa, acceso directo desde notebooks y herramientas BI.| An√°lisis, reporting, dashboards, colaboraci√≥n entre equipos.     |\n",
    "\n",
    "Ambos m√©todos aprovechan las ventajas de Delta Lake: transacciones ACID, versionado, rendimiento y escalabilidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c236143a-80ec-4618-bac0-392565c42bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Modificar una tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2b397e7-e0ab-4aeb-9a9d-dadfcdc6ffc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cuando trabajamos con tablas Delta, una de las grandes ventajas es la posibilidad de realizar operaciones transaccionales complejas de forma eficiente y segura como:\n",
    "- Updates\n",
    "- Deletes\n",
    "- Merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "674a6faf-c250-4bd4-8401-998a348506ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, base_path+\"delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e6e12b7-2f39-4832-be71-f40edac27719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae7f84c5-cdfa-402b-be8b-345b99f5adf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Permite modificar el valor de una o varias columnas en las filas que cumplen una condici√≥n espec√≠fica. Por ejemplo, se puede actualizar el nombre de un producto o corregir un valor err√≥neo en una tabla sin tener que reescribir todo el dataset. La sintaxis es similar a la de SQL, pero se realiza sobre la API de DeltaTable en Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5133cb3-045a-444a-8504-945e97742eab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UPDATE\n",
    "delta_table.update(\n",
    "    condition=\"id = 1\",\n",
    "    set={\"producto\": \"'Ordenador'\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "771df4e3-4e51-44c1-ad88-f50140f82c83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0d94f00-61df-4472-b7bd-8b7db04540d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Permite eliminar filas que cumplen una condici√≥n determinada. Es √∫til para depurar datos, eliminar registros obsoletos o cumplir con requisitos legales de borrado. Al igual que el update, el delete es transaccional y garantiza la integridad de la tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99d6c840-f27c-4540-b4e3-bf8cdb8b6d96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DELETE\n",
    "delta_table.delete(\"precio > 150\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0c8f01e-7aee-4cf4-9038-b09969d1b0a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ambas operaciones aprovechan las transacciones ACID de Delta Lake, lo que significa que los cambios son at√≥micos, consistentes, aislados y duraderos. Esto evita problemas de concurrencia y asegura que los datos siempre est√©n en un estado v√°lido, incluso en entornos multiusuario o de procesamiento distribuido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fe5968d-b9c8-4870-9f88-aa4b538e4237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78817581-894e-4a7a-8f40-1d940cc430ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "En este bloque de c√≥digo se muestra c√≥mo realizar un \"merge\" (tambi√©n conocido como upsert) sobre una tabla Delta:\n",
    "\n",
    "1. Se crea un DataFrame con nuevos datos o datos actualizados.\n",
    "2. Luego, se utiliza el m√©todo `merge` de la API de Delta Lake para comparar los datos existentes en la tabla (target) con los nuevos datos (source) usando una condici√≥n de emparejamiento (en este caso, el campo id).\n",
    "3. Si el `id` ya existe en la tabla, se actualizan todos los campos de ese registro (`whenMatchedUpdateAll`).\n",
    "4. Si el `id` no existe, se inserta el nuevo registro (`whenNotMatchedInsertAll`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a5f4028-e032-4b25-9bb4-331a8b84bc45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Nuevos datos a insertar/actualizar\n",
    "columns = [\"id\", \"fecha\", \"producto\", \"cantidad\", \"precio\"]\n",
    "\n",
    "nuevos_datos = [(3, \"2025-05-24\", \"Monitor\", 1, 179.99), (4, \"2025-05-24\", \"Impresora\", 2, 89.99)]\n",
    "df_updates = spark.createDataFrame(nuevos_datos, columns)\n",
    "\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    df_updates.alias(\"source\"),\n",
    "    \"target.id = source.id\") \\\n",
    "  .whenMatchedUpdateAll() \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d3907cb-6fc3-45e0-a3d2-938d5dd13cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Este tipo de operaci√≥n es fundamental en escenarios de integraci√≥n incremental de datos, donde peri√≥dicamente llegan nuevos registros o actualizaciones y queremos mantener la tabla Delta siempre actualizada y sin duplicados.\n",
    "\n",
    "Ventajas de usar merge en Delta Lake:\n",
    "\n",
    "- Permite mantener la integridad y consistencia de los datos.\n",
    "- Facilita la implementaci√≥n de pipelines de datos incrementales.\n",
    "- Aprovecha las transacciones ACID de Delta Lake, evitando problemas de concurrencia o corrupci√≥n de datos.\n",
    "- Es mucho m√°s eficiente y sencillo que realizar operaciones manuales de actualizaci√≥n e inserci√≥n por separado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fb8d71f-6822-47d8-a405-a66a3736db90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Time Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9efa0af-2983-44ea-aeb1-e037e235ae75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "El Time Travel en Delta Lake es una funcionalidad que permite consultar versiones anteriores de una tabla Delta. Cada vez que se realiza una operaci√≥n de escritura (insert, update, delete, merge), Delta Lake crea una nueva versi√≥n de la tabla, manteniendo el historial de cambios.\n",
    "\n",
    "**¬øPara qu√© sirve el Time Travel?**\n",
    "- Recuperar datos borrados o modificados accidentalmente.\n",
    "- Auditar cambios y analizar c√≥mo han evolucionado los datos a lo largo del tiempo.\n",
    "- Comparar el estado de la tabla en diferentes momentos.\n",
    "- Reproducir experimentos o an√°lisis sobre datos hist√≥ricos.\n",
    "\n",
    "**¬øC√≥mo se usa?**\n",
    "Puedes acceder a una versi√≥n anterior de la tabla especificando el n√∫mero de versi√≥n (`versionAsOf`) o una marca de tiempo (`timestampAsOf`) al leer los datos:\n",
    "\n",
    "**Ventajas:**\n",
    "- No necesitas mantener copias manuales de los datos para auditor√≠a o recuperaci√≥n.\n",
    "- Todas las operaciones de Time Travel son transaccionales y consistentes.\n",
    "- Facilita la trazabilidad y el cumplimiento normativo en entornos empresariales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcaf881c-9979-4ec0-a59b-73a50fa6b7aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(delta_table.toDF())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "329572e4-83ca-4a27-99db-d197e750fb9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ver historial\n",
    "display(delta_table.history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebb96718-f650-46be-8435-4b2f7f622a34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer versi√≥n anterior\n",
    "df_old = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(base_path+\"delta\")\n",
    "display(df_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f7d004f-8635-4507-b296-4e5efcdfbae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer la tabla tal como estaba en una fecha concreta\n",
    "df_moment = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2025-05-27 10:00:00\").load(base_path+\"delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17d363db-6c02-473b-a4a9-2009751239d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Ejercicio Guiado Time Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "426c7681-605d-4e25-8dc1-1f5b1b605130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Seguir los pasos de las siguientes celdas para llegar a ver el resultado final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "225eeedd-26d0-478f-b1d2-5d90a9a877d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 1: Crear una df inicial con datos de empleados\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema_empleados = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"nombre\", StringType(), False),\n",
    "    StructField(\"departamento\", StringType(), False),\n",
    "    StructField(\"salario\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "datos_iniciales = [\n",
    "    (1, \"Ana Garc√≠a\", \"Ventas\", 35000),\n",
    "    (2, \"Carlos L√≥pez\", \"IT\", 45000),\n",
    "    (3, \"Mar√≠a Ruiz\", \"Marketing\", 38000)\n",
    "]\n",
    "\n",
    "### Completa el c√≥digo para crear el DataFrame con el esquema definido\n",
    "df_empleados = spark.createDataFrame(.....)\n",
    "display(df_empleados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9edcfc42-820a-4c74-906b-f586c59c43d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 2: Guardar el DataFrame como tabla Delta\n",
    "path_empleados = base_path + \"empleados_delta\"\n",
    "\n",
    "df_empleados.write....\n",
    "\n",
    "print(\"‚úÖ Tabla creada y guardada en:\", path_empleados)\n",
    "print(\"\\nüìç Para verificar que la tabla existe, ve a:\")\n",
    "print(\"   - Databricks UI > Data > DBFS > Volumes > workspace > ceste > archivos > empleados_delta\")\n",
    "print(\"   - O ejecuta: dbutils.fs.ls(base_path + 'empleados_delta')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90c03e5c-8ec3-4bfe-88d3-f7c38fdbb56d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 3: Realizar una operaci√≥n de actualizaci√≥n\n",
    "print(\"\\nüîÑ Ahora vamos a actualizar el salario de Carlos L√≥pez...\")\n",
    "\n",
    "delta_empleados = DeltaTable.forPath(spark, path_empleados)\n",
    "delta_empleados.update(\n",
    "    condition=\"nombre = 'Carlos L√≥pez'\",\n",
    "    set={\"salario\": \"50000\"}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Salario actualizado de 45000 a 50000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8620ad1b-5c46-4bf2-82f2-cd1c50553f4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 4 Verificar el cambio. Para ello vuelve a leer la tabla Delta\n",
    "df_actualizado = spark.read....\n",
    "print(\"\\nüìä Datos actuales de la tabla:\")\n",
    "display(df_actualizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e380f2f0-7fd4-4aea-9acd-c6e7a462788b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 5: Mostrar el historial de versiones\n",
    "print(\"\\nüìú Historial de versiones de la tabla:\")\n",
    "print(\"Ejecuta el siguiente c√≥digo para ver todas las operaciones realizadas:\\n\")\n",
    "print(\"display(delta_empleados.history())\")\n",
    "print(\"\\nüí° Deber√≠as ver 2 versiones:\")\n",
    "print(\"   - Versi√≥n 0: Creaci√≥n inicial de la tabla (WRITE)\")\n",
    "print(\"   - Versi√≥n 1: Actualizaci√≥n del salario (UPDATE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3150520-303b-47b8-844b-e79d44edce81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Paso 6: Comparar versiones usando Time Travel\n",
    "print(\"\\n‚è∞ Time Travel: Comparando versi√≥n actual vs versi√≥n inicial\")\n",
    "\n",
    "df_version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(path_empleados)\n",
    "df_version_1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(path_empleados)\n",
    "\n",
    "print(\"\\nüìå Versi√≥n 0 (Estado inicial):\")\n",
    "display(df_version_0)\n",
    "\n",
    "print(\"\\nüìå Versi√≥n 1 (Despu√©s de la actualizaci√≥n):\")\n",
    "display(df_version_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cca50b3-3c50-4550-ade4-fad53cab7b85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Optimizaci√≥n de Tablas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf9bd902-5d03-437d-a841-62e5b49ff32c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "La optimizaci√≥n de tablas en Delta Lake es clave para mejorar el rendimiento de las consultas y reducir el coste de almacenamiento en entornos de Big Data. Existen dos t√©cnicas principales:\n",
    "\n",
    "- **Compactaci√≥n (OPTIMIZE):** Consiste en reducir el n√∫mero de archivos peque√±os que se generan tras m√∫ltiples escrituras o actualizaciones. Al compactar, se agrupan estos archivos en otros m√°s grandes, lo que acelera las lecturas y reduce la sobrecarga de gesti√≥n de archivos en el sistema distribuido.\n",
    "\n",
    "- **Z-Ordering:** Es una t√©cnica de ordenaci√≥n f√≠sica de los datos en disco basada en una o varias columnas clave. Al aplicar Z-Ordering, los datos se almacenan de forma que las filas con valores similares en las columnas seleccionadas queden f√≠sicamente pr√≥ximas. Esto mejora notablemente el rendimiento de las consultas filtradas por esas columnas, ya que minimiza la cantidad de datos que Spark necesita leer.\n",
    "\n",
    "**Ventajas de la optimizaci√≥n:**\n",
    "- Consultas m√°s r√°pidas y eficientes, especialmente en grandes vol√∫menes de datos.\n",
    "- Menor latencia en dashboards y an√°lisis interactivos.\n",
    "- Reducci√≥n de costes de almacenamiento y procesamiento.\n",
    "- Mejor aprovechamiento de los recursos del cluster.\n",
    "\n",
    "**Cu√°ndo optimizar:**\n",
    "- Tras cargas masivas de datos o procesos ETL frecuentes.\n",
    "- Cuando se detecta degradaci√≥n en el rendimiento de las consultas.\n",
    "- Antes de ejecutar an√°lisis cr√≠ticos o dashboards de negocio.\n",
    "\n",
    "En resumen, la optimizaci√≥n peri√≥dica de las tablas Delta es una buena pr√°ctica para mantener el entorno √°gil, eficiente y escalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3155b46c-8239-4cea-9657-dbfd64b2544d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optimizar tabla\n",
    "spark.sql(\"OPTIMIZE ceste.productos\")\n",
    "# Ordenar f√≠sicamente por \"id\"\n",
    "spark.sql(\"OPTIMIZE ceste.productos ZORDER BY id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "838b274e-d82a-4414-8d13-8742126efc2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3fe45b3-9138-45e0-8117-92d7ef9fa38f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Crea un DF con datos de clientes. Para ello puedes utilizar la informaci√≥n que te doy y la funcion `list(zip(X,Y))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c95794b5-6a15-48e3-bef4-406a108788fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nombres = [\"Juan\", \"Mar√≠a\", \"Carlos\", \"Ana\", \"Luis\", \"Carmen\", \"Jos√©\", \"Laura\", \"Pedro\", \"Luc√≠a\", \"Miguel\", \"Elena\", \"Javier\", \"Sof√≠a\", \"Antonio\", \"Marta\", \"Manuel\", \"Isabel\", \"Francisco\", \"Patricia\"]\n",
    "edades = [25, 30, 22, 28, 35, 27, 40, 32, 24, 29, 33, 26, 31, 23, 36, 21, 34, 38, 37, 20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac8cc155-75d6-46a2-b443-47497fb9c33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. A√±ade la columna indice. Para ello ayudate de la funcion `monotonically_increasing_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52d32569-db8f-46d6-a87e-a79790d59fcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4913ace3-765d-4878-9220-c667ed54f076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Guardar como Delta Table (por path). Utiliza la variable `base_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3650652c-74d8-47fd-bc2f-ef893f56e22b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "448806e9-ff37-45a5-92f3-b12e5dbbe1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Actualiza la edad de un cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "838f509a-fa58-4e3d-bf34-35e19a22230b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77ac0079-a76c-4ef9-ad5c-9472066881d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Insertar un nuevo cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "612a6f43-66be-4c2c-9b51-8be1ee2bce0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b89b238-c48a-4cd6-aaeb-ab42d9722131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c4b5b08-79c3-403f-ad69-c6836856668b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Consulta el historial de la tabla. Posteriormente lee una version anterior con Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13d36127-ff94-47b6-84c9-7d5fee793af7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ver historial de cambios\n",
    "\n",
    "\n",
    "# Leer la versi√≥n inicial (version 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2a24a3b-0dc6-4ad7-9f57-e7dec73558aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Registrar como tabla del metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d44f5755-0b69-49fc-8562-2db80b7cb5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3e086c3-3a35-4d5a-bb17-d504796a3dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Borra todos los registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "097534d4-fd36-4cb6-9d9f-6061eb21d0dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7d8c2f2-ae49-4742-9995-63c658d5ea56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6874ac0-8da8-49d8-b4ae-4da8205488b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "9. Usa widgets para seleccionar din√°micamente la tabla Delta que creamos al principio del notebook (\"delta\") y mostrar su contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff395ba6-6ea6-4afc-a773-4b6d3176c75d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf3f5b43-ff0e-4bb3-af78-d94bd20ed4a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86a12d0d-28b3-4a38-8eeb-48a6d9bda88a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "10. Time travel con widgets, utiliza un widget tipo texto para poder leer dinamicamente la primera version de la tabla delta inicial (\"delta\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c255e2e-eb93-4335-b2f1-654dfe6608cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4104c3d-e882-47a3-8ea8-ae056bffc375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23d972be-b146-4b4c-b204-e5bf8ebdf19d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "11. Atomicidad de Delta Table: Haz una demostracion de la atomicidad en delta Tables, prepara una insercion de dos filas donde la primera vaya a ser correcta y la segunda erronea. Seguidamente comprueba que no se inserto nada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cca0e21-fe4a-43b7-85a2-e32fc396b29c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Este batch contiene un error: la segunda fila tiene un precio como string\n",
    "data_erronea = [\n",
    "    (\"10\", \"2025-05-28\", \"Tablet\", 2, 299.99),\n",
    "    (\"11\", \"2025-05-28\", \"Teclado\", 3, \"caro\")  # Error intencionado\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42d2245e-2671-4ed8-9ddf-63885e62c7f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39950d63-90de-4181-b89a-75fefa13a23a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "12. exceptAll() - Permite comparar 2 versiones de dfs y ver sus diferencias.\n",
    "\n",
    "carga dos versiones de tu tabla delta en dos dfs distintos y aplicalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08c8cc1a-4fb5-416a-926e-9ec57ed6e9f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_actual=\n",
    "df_anterior=\n",
    "\n",
    "\n",
    "\n",
    "df_actual.exceptAll(df_anterior).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8441015608548784,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Databricks_ ETL y Delta Lake",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "ceste",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
