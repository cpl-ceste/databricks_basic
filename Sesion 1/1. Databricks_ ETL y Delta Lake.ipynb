{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efb84b5f-2ddc-47ab-a8cc-59c78c783285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks: ETLs y Delta Lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdbd4652-db73-44e5-89cc-6d9253da6a92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ¿Qué es un ETL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d1e1c9a-3b95-4383-8054-9f19e73de922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Un ETL (Extract, Transform, Load) es un proceso fundamental en la ingeniería de datos. Consiste en extraer datos de diferentes fuentes, transformarlos para adecuarlos a las necesidades del negocio y cargarlos en un sistema de almacenamiento o análisis, como un Data Lake o un Data Warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8b60ccf-9427-430f-a6b9-47ef21d98d34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### dbutils\n",
    "`dbutils` es una colección de utilidades proporcionadas por Databricks para interactuar con el entorno, gestionar archivos, parámetros y flujos de trabajo. Es muy útil para la gestión de datos y automatización de tareas dentro de notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf1ffea-9c2b-41f0-b5b6-e15b1fdbe997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83405a7e-0fa7-4104-a8a2-3ccc20d4d640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Dentro del catálogo de `dbutils` existen subsecciones como:\n",
    " - dbutils.fs: Para interactuar con el sistema de archivos\n",
    " - dbutils.secrets: Para usar secretos\n",
    " - dbutils.notebook: Para poder interactuar con otros notebooks\n",
    " - dbutils.widgets: Para poder parametrizar los notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4cfe470-f801-44d8-afc2-9867098e848f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"dbfs:/Volumes/workspace/ceste/archivos/\"\n",
    "dbutils.fs.ls(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3db3871-5f2c-4ac9-8bbb-3db76f54fa87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingesta desde Distintos Orígenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbb408d9-dcc8-467a-a410-ab737706661f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "En los entornos de Big Data, los datos pueden venir en múltiples formatos: CSV, Parquet, JSON, entre otros. Cada formato tiene ventajas y desventajas en cuanto a compresión, velocidad de lectura/escritura y compatibilidad. Parquet, por ejemplo, es columnar y eficiente para grandes volúmenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "767c754b-2cf6-4e1e-a9df-1cc3dc31dca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**IMPORTANTE**: Antes de seguir subir a Databricks los ficheros del repositorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d73bcc61-ceca-4d4b-b049-45b89084df61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Leer desde CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da8f6013-6bde-4d4c-8baa-37cbbccb392a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Leer datos desde un archivo CSV es una de las formas más comunes de ingesta en proyectos de datos. El formato CSV (Comma Separated Values) es ampliamente utilizado por su simplicidad y compatibilidad con la mayoría de las herramientas. Sin embargo, es importante tener en cuenta que no es el formato más eficiente para grandes volúmenes de datos, ya que no soporta tipos de datos complejos ni compresión nativa. Por eso, en entornos de Big Data, a menudo se prefiere convertir los datos a formatos como Parquet o Delta una vez cargados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "764443d7-9a67-45a9-9f0e-4a02c1b834d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv = spark.read.csv(\n",
    "    path=base_path+\"ventas.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d85a08b5-0144-4473-914e-01b1848f4480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_csv.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62d48cb5-6bd3-431b-bbb7-b850e31ef9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Leer desde parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "981cc141-81ac-4f40-bb1f-43ce51f41d25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "El formato Parquet es un estándar de almacenamiento columnar ampliamente utilizado en Big Data. Su principal fortaleza es la eficiencia tanto en almacenamiento como en velocidad de lectura, especialmente cuando se trabaja con grandes volúmenes de datos y consultas sobre columnas específicas. Parquet permite compresión y soporta tipos de datos complejos, lo que lo hace ideal para análisis y procesamiento distribuido. Por ello, es habitual convertir datos de formatos como CSV a Parquet para optimizar el rendimiento y reducir costes en proyectos de análisis de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e27776-310a-4f14-9785-83c9418b5d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(base_path+\"ventas.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "499ff56a-e6ad-4bec-a4e7-3afa2789f67e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfdc0227-aefa-4788-b566-d9e320b3ed95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Leer desde Json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93f12daf-6dd8-47ba-9ffa-0cc526e37034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "El formato JSON (JavaScript Object Notation) es ampliamente utilizado para el intercambio de datos debido a su flexibilidad y legibilidad. Permite almacenar estructuras de datos complejas, como listas y diccionarios anidados. Sin embargo, en comparación con Parquet, JSON no es tan eficiente en almacenamiento ni en velocidad de procesamiento para grandes volúmenes de datos. Es útil cuando se requiere flexibilidad en la estructura de los datos o cuando los datos provienen de APIs y sistemas web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2090c386-4402-4ff4-be90-f95eda06cd54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json = spark.read.json(base_path+\"ventas.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cef43e1-1a43-4cd7-b718-0496bad5e2ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9026c06-aa32-40e6-804e-cae5f43f2e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Leer desde BD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e644065-fa5d-44c8-b346-aab926e018fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "La lectura de datos desde bases de datos (BD) es fundamental cuando los datos se encuentran almacenados en sistemas transaccionales o relacionales, como MySQL, SQL Server, PostgreSQL, entre otros. En entornos de Big Data, es común extraer datos de estas fuentes para integrarlos en un Data Lake o procesarlos con Spark. La conexión suele realizarse mediante JDBC, permitiendo ejecutar consultas SQL y cargar los resultados como DataFrames. Es importante considerar la seguridad, el rendimiento y la gestión de credenciales al conectar con bases de datos externas.\n",
    "\n",
    "*La siguiente celda no va a hacer nada porque para funcionar necesitaríamos una de BD activa*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d82d8e13-8128-4d4f-8f67-29c09e1fbd01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:mysql://<host>:<port>/<database>\"\n",
    "properties = {\"user\": \"user\", \"password\": \"pwd\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6330c05-f3dd-4994-86f2-8c7dd5436160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Recordatorio: Leer con/sin esquema\n",
    "\n",
    "Al leer datos en Spark, se puede dejar que el sistema infiera automáticamente el esquema (tipos de datos de cada columna) o definirlo explícitamente. Inferir el esquema es cómodo y rápido para exploraciones iniciales, pero puede ser más lento y propenso a errores si los datos son inconsistentes o si hay muchas columnas. Definir el esquema manualmente garantiza que los tipos de datos sean los esperados, mejora el rendimiento en la carga y ayuda a evitar problemas en etapas posteriores del procesamiento. Es una buena práctica definir el esquema en entornos productivos o cuando se requiere mayor control y robustez sobre los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f154f45-8778-414a-9b1d-4e6dcd667682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Definir el esquema manualmente\n",
    "schema = StructType([\n",
    "    # Nombre, Tipo de dato, Requerido\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"fecha\", StringType(), True),\n",
    "    StructField(\"producto\", StringType(), True),\n",
    "    StructField(\"cantidad\", IntegerType(), True),\n",
    "    StructField(\"precio\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_csv = spark.read.csv(\n",
    "    path=base_path+\"ventas.csv\",\n",
    "    header=True,\n",
    "    schema=schema,\n",
    "    sep=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2596a697-52a2-4bec-967a-94ab25360c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17609ccb-8ded-4efa-b6a1-8e42ae89fb7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef950c5c-16c7-43da-a2c7-882a33778a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ¿Qué es Delta Lake?\n",
    "Delta Lake es una capa de almacenamiento open source que se integra con Apache Spark y añade capacidades ACID (Atomicidad, Consistencia, Aislamiento, Durabilidad) a los Data Lakes. Permite versionado, time travel, y operaciones transaccionales sobre los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "986a7c81-3667-40e4-988b-c855438c9dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ¿Por qué es tan valioso el formato de Delta Table?\n",
    "Guardar tablas en formato Delta permite aprovechar las ventajas de transacciones ACID, manejo de versiones, y optimización de consultas. Es ideal para entornos donde los datos cambian frecuentemente y se requiere trazabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fe29e46-ed26-45c4-9b4d-95c54f3010cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Una Delta Table permite realizar operaciones ACID, mantener el histórico con time travel, gestionar versiones, optimizar el almacenamiento, y escalar en entornos de producción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05acd30a-9588-4859-ae93-2a5b31351f4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Guardar una tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c95a1e30-ec56-4ebf-83da-71268aab5d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Guardar una tabla en Databricks puede hacerse de dos formas principales:\n",
    "\n",
    "- Por path: Consiste en guardar los datos directamente en una ruta específica del sistema de archivos (por ejemplo, en DBFS) usando el formato Delta. Esta opción es flexible y permite gestionar los archivos de manera directa, moverlos entre entornos o integrarlos con otros sistemas que acceden por ruta.\n",
    "\n",
    "- Por catálogo del metastore: Permite registrar la tabla en el catálogo de Databricks, lo que facilita su consulta mediante SQL y su integración con herramientas como Unity Catalog. Esta opción es ideal para entornos colaborativos, ya que permite a varios usuarios acceder a la misma tabla de forma controlada y segura, además de aprovechar funcionalidades avanzadas como el control de versiones, permisos y auditoría.\n",
    "\n",
    "Ambas opciones aprovechan las ventajas del formato Delta, como las transacciones ACID, el versionado y la optimización de consultas. La elección entre una u otra depende de las necesidades de gestión, seguridad y acceso en el proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93f580ba-290a-4ea3-859d-7d144eb9de49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Por path\n",
    "df_csv.write.format(\"delta\").mode(\"overwrite\").save(base_path+\"delta\")\n",
    "\n",
    "# Por catalogo\n",
    "df_csv.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ceste.productos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb24fd8c-e562-4873-a4a1-e7f6a3f9ff7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Leer una tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d2fa14b-ccee-4aa6-9f4a-ab5ac9024743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Lectura de una tabla Delta desde Spark: También puedes leer una tabla Delta directamente desde Spark usando el API de DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c990b0d-09e7-477a-a64a-dbfd5eefca54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_delta = spark.read.format(\"delta\").load(base_path+\"delta\")\n",
    "df_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60fbf1a4-8b58-4ddb-8a86-f461173878f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Esto es útil cuando necesitas manipular los datos con Python, realizar transformaciones complejas, aplicar lógica de negocio o integrarlo en pipelines de procesamiento.\n",
    "\n",
    "2. Consulta SQL sobre una tabla Delta: Puedes consultar una tabla Delta registrada en el catálogo usando SQL estándar. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09949e50-5c80-4fe8-b964-1893f55c5d2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM ceste.productos;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5f25b95-c501-4586-9c7c-5bc5f84d9065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Esto te permite aprovechar toda la potencia del lenguaje SQL para filtrar, agrupar, unir y analizar los datos almacenados en formato Delta. Es especialmente útil para usuarios que prefieren trabajar con SQL o para integraciones con herramientas de BI.\n",
    "\n",
    "Diferencias y ventajas:\n",
    "- Consultar por SQL es ideal para análisis exploratorio, dashboards y colaboración entre equipos.\n",
    "- Leer con Spark DataFrame es más flexible para procesamiento avanzado, machine learning y automatización.\n",
    "- Ambas formas aprovechan las ventajas de Delta Lake: transacciones ACID, versionado, rendimiento y escalabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c236143a-80ec-4618-bac0-392565c42bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Modificar una tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2b397e7-e0ab-4aeb-9a9d-dadfcdc6ffc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cuando trabajamos con tablas Delta, una de las grandes ventajas es la posibilidad de realizar operaciones transaccionales complejas de forma eficiente y segura como:\n",
    "- Updates\n",
    "- Deletes\n",
    "- Merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "674a6faf-c250-4bd4-8401-998a348506ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, base_path+\"delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e6e12b7-2f39-4832-be71-f40edac27719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae7f84c5-cdfa-402b-be8b-345b99f5adf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Permite modificar el valor de una o varias columnas en las filas que cumplen una condición específica. Por ejemplo, se puede actualizar el nombre de un producto o corregir un valor erróneo en una tabla sin tener que reescribir todo el dataset. La sintaxis es similar a la de SQL, pero se realiza sobre la API de DeltaTable en Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5133cb3-045a-444a-8504-945e97742eab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UPDATE\n",
    "delta_table.update(\n",
    "    condition=\"id = 1\",\n",
    "    set={\"producto\": \"'Ordenador'\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "771df4e3-4e51-44c1-ad88-f50140f82c83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0d94f00-61df-4472-b7bd-8b7db04540d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Permite eliminar filas que cumplen una condición determinada. Es útil para depurar datos, eliminar registros obsoletos o cumplir con requisitos legales de borrado. Al igual que el update, el delete es transaccional y garantiza la integridad de la tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99d6c840-f27c-4540-b4e3-bf8cdb8b6d96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DELETE\n",
    "delta_table.delete(\"precio > 150\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0c8f01e-7aee-4cf4-9038-b09969d1b0a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ambas operaciones aprovechan las transacciones ACID de Delta Lake, lo que significa que los cambios son atómicos, consistentes, aislados y duraderos. Esto evita problemas de concurrencia y asegura que los datos siempre estén en un estado válido, incluso en entornos multiusuario o de procesamiento distribuido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fe5968d-b9c8-4870-9f88-aa4b538e4237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78817581-894e-4a7a-8f40-1d940cc430ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "En este bloque de código se muestra cómo realizar un \"merge\" (también conocido como upsert) sobre una tabla Delta:\n",
    "\n",
    "1. Se crea un DataFrame con nuevos datos o datos actualizados.\n",
    "2. Luego, se utiliza el método `merge` de la API de Delta Lake para comparar los datos existentes en la tabla (target) con los nuevos datos (source) usando una condición de emparejamiento (en este caso, el campo id).\n",
    "3. Si el `id` ya existe en la tabla, se actualizan todos los campos de ese registro (`whenMatchedUpdateAll`).\n",
    "4. Si el `id` no existe, se inserta el nuevo registro (`whenNotMatchedInsertAll`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a5f4028-e032-4b25-9bb4-331a8b84bc45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Nuevos datos a insertar/actualizar\n",
    "columns = [\"id\", \"fecha\", \"producto\", \"cantidad\", \"precio\"]\n",
    "\n",
    "nuevos_datos = [(3, \"2025-05-24\", \"Monitor\", 1, 179.99), (4, \"2025-05-24\", \"Impresora\", 2, 89.99)]\n",
    "df_updates = spark.createDataFrame(nuevos_datos, columns)\n",
    "\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    df_updates.alias(\"source\"),\n",
    "    \"target.id = source.id\") \\\n",
    "  .whenMatchedUpdateAll() \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d3907cb-6fc3-45e0-a3d2-938d5dd13cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Este tipo de operación es fundamental en escenarios de integración incremental de datos, donde periódicamente llegan nuevos registros o actualizaciones y queremos mantener la tabla Delta siempre actualizada y sin duplicados.\n",
    "\n",
    "Ventajas de usar merge en Delta Lake:\n",
    "\n",
    "- Permite mantener la integridad y consistencia de los datos.\n",
    "- Facilita la implementación de pipelines de datos incrementales.\n",
    "- Aprovecha las transacciones ACID de Delta Lake, evitando problemas de concurrencia o corrupción de datos.\n",
    "- Es mucho más eficiente y sencillo que realizar operaciones manuales de actualización e inserción por separado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fb8d71f-6822-47d8-a405-a66a3736db90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Time Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9efa0af-2983-44ea-aeb1-e037e235ae75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "El Time Travel en Delta Lake es una funcionalidad que permite consultar versiones anteriores de una tabla Delta. Cada vez que se realiza una operación de escritura (insert, update, delete, merge), Delta Lake crea una nueva versión de la tabla, manteniendo el historial de cambios.\n",
    "\n",
    "**¿Para qué sirve el Time Travel?**\n",
    "- Recuperar datos borrados o modificados accidentalmente.\n",
    "- Auditar cambios y analizar cómo han evolucionado los datos a lo largo del tiempo.\n",
    "- Comparar el estado de la tabla en diferentes momentos.\n",
    "- Reproducir experimentos o análisis sobre datos históricos.\n",
    "\n",
    "**¿Cómo se usa?**\n",
    "Puedes acceder a una versión anterior de la tabla especificando el número de versión (`versionAsOf`) o una marca de tiempo (`timestampAsOf`) al leer los datos:\n",
    "\n",
    "**Ventajas:**\n",
    "- No necesitas mantener copias manuales de los datos para auditoría o recuperación.\n",
    "- Todas las operaciones de Time Travel son transaccionales y consistentes.\n",
    "- Facilita la trazabilidad y el cumplimiento normativo en entornos empresariales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcaf881c-9979-4ec0-a59b-73a50fa6b7aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(delta_table.toDF())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "329572e4-83ca-4a27-99db-d197e750fb9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ver historial\n",
    "display(delta_table.history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebb96718-f650-46be-8435-4b2f7f622a34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer versión anterior\n",
    "df_old = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(base_path+\"delta\")\n",
    "display(df_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f7d004f-8635-4507-b296-4e5efcdfbae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer la tabla tal como estaba en una fecha concreta\n",
    "df_moment = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2025-05-27 10:00:00\").load(base_path+\"delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cca50b3-3c50-4550-ade4-fad53cab7b85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Optimización de Tablas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf9bd902-5d03-437d-a841-62e5b49ff32c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "La optimización de tablas en Delta Lake es clave para mejorar el rendimiento de las consultas y reducir el coste de almacenamiento en entornos de Big Data. Existen dos técnicas principales:\n",
    "\n",
    "- **Compactación (OPTIMIZE):** Consiste en reducir el número de archivos pequeños que se generan tras múltiples escrituras o actualizaciones. Al compactar, se agrupan estos archivos en otros más grandes, lo que acelera las lecturas y reduce la sobrecarga de gestión de archivos en el sistema distribuido.\n",
    "\n",
    "- **Z-Ordering:** Es una técnica de ordenación física de los datos en disco basada en una o varias columnas clave. Al aplicar Z-Ordering, los datos se almacenan de forma que las filas con valores similares en las columnas seleccionadas queden físicamente próximas. Esto mejora notablemente el rendimiento de las consultas filtradas por esas columnas, ya que minimiza la cantidad de datos que Spark necesita leer.\n",
    "\n",
    "**Ventajas de la optimización:**\n",
    "- Consultas más rápidas y eficientes, especialmente en grandes volúmenes de datos.\n",
    "- Menor latencia en dashboards y análisis interactivos.\n",
    "- Reducción de costes de almacenamiento y procesamiento.\n",
    "- Mejor aprovechamiento de los recursos del cluster.\n",
    "\n",
    "**Cuándo optimizar:**\n",
    "- Tras cargas masivas de datos o procesos ETL frecuentes.\n",
    "- Cuando se detecta degradación en el rendimiento de las consultas.\n",
    "- Antes de ejecutar análisis críticos o dashboards de negocio.\n",
    "\n",
    "En resumen, la optimización periódica de las tablas Delta es una buena práctica para mantener el entorno ágil, eficiente y escalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3155b46c-8239-4cea-9657-dbfd64b2544d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optimizar tabla\n",
    "spark.sql(\"OPTIMIZE ceste.productos\")\n",
    "# Ordenar físicamente por \"id\"\n",
    "spark.sql(\"OPTIMIZE ceste.productos ZORDER BY id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "838b274e-d82a-4414-8d13-8742126efc2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3fe45b3-9138-45e0-8117-92d7ef9fa38f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Crea un DF con datos de clientes. Para ello puedes utilizar la información que te doy y la funcion `list(zip(X,Y))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c95794b5-6a15-48e3-bef4-406a108788fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nombres = [\"Juan\", \"María\", \"Carlos\", \"Ana\", \"Luis\", \"Carmen\", \"José\", \"Laura\", \"Pedro\", \"Lucía\", \"Miguel\", \"Elena\", \"Javier\", \"Sofía\", \"Antonio\", \"Marta\", \"Manuel\", \"Isabel\", \"Francisco\", \"Patricia\"]\n",
    "edades = [25, 30, 22, 28, 35, 27, 40, 32, 24, 29, 33, 26, 31, 23, 36, 21, 34, 38, 37, 20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac8cc155-75d6-46a2-b443-47497fb9c33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Añade la columna indice. Para ello ayudate de la funcion `monotonically_increasing_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52d32569-db8f-46d6-a87e-a79790d59fcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4913ace3-765d-4878-9220-c667ed54f076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Guardar como Delta Table (por path). Utiliza la variable `base_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3650652c-74d8-47fd-bc2f-ef893f56e22b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "448806e9-ff37-45a5-92f3-b12e5dbbe1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Actualiza la edad de un cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "838f509a-fa58-4e3d-bf34-35e19a22230b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77ac0079-a76c-4ef9-ad5c-9472066881d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Insertar un nuevo cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "612a6f43-66be-4c2c-9b51-8be1ee2bce0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b89b238-c48a-4cd6-aaeb-ab42d9722131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c4b5b08-79c3-403f-ad69-c6836856668b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Consulta el historial de la tabla. Posteriormente lee una version anterior con Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13d36127-ff94-47b6-84c9-7d5fee793af7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ver historial de cambios\n",
    "\n",
    "\n",
    "# Leer la versión inicial (version 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2a24a3b-0dc6-4ad7-9f57-e7dec73558aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Registrar como tabla del metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d44f5755-0b69-49fc-8562-2db80b7cb5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3e086c3-3a35-4d5a-bb17-d504796a3dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Borra todos los registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "097534d4-fd36-4cb6-9d9f-6061eb21d0dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7d8c2f2-ae49-4742-9995-63c658d5ea56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6874ac0-8da8-49d8-b4ae-4da8205488b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "9. Usa widgets para seleccionar dinámicamente la tabla Delta que creamos al principio del notebook (\"delta\") y mostrar su contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff395ba6-6ea6-4afc-a773-4b6d3176c75d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf3f5b43-ff0e-4bb3-af78-d94bd20ed4a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86a12d0d-28b3-4a38-8eeb-48a6d9bda88a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "10. Time travel con widgets, utiliza un widget tipo texto para poder leer dinamicamente la primera version de la tabla delta inicial (\"delta\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c255e2e-eb93-4335-b2f1-654dfe6608cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4104c3d-e882-47a3-8ea8-ae056bffc375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23d972be-b146-4b4c-b204-e5bf8ebdf19d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "11. Atomicidad de Delta Table: Haz una demostracion de la atomicidad en delta Tables, prepara una insercion de dos filas donde la primera vaya a ser correcta y la segunda erronea. Seguidamente comprueba que no se inserto nada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cca0e21-fe4a-43b7-85a2-e32fc396b29c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Este batch contiene un error: la segunda fila tiene un precio como string\n",
    "data_erronea = [\n",
    "    (\"10\", \"2025-05-28\", \"Tablet\", 2, 299.99),\n",
    "    (\"11\", \"2025-05-28\", \"Teclado\", 3, \"caro\")  # Error intencionado\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42d2245e-2671-4ed8-9ddf-63885e62c7f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39950d63-90de-4181-b89a-75fefa13a23a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "12. exceptAll() - Permite comparar 2 versiones de dfs y ver sus diferencias.\n",
    "\n",
    "carga dos versiones de tu tabla delta en dos dfs distintos y aplicalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08c8cc1a-4fb5-416a-926e-9ec57ed6e9f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_actual=\n",
    "df_anterior=\n",
    "\n",
    "\n",
    "\n",
    "df_actual.exceptAll(df_anterior).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7644664719660558,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Databricks_ ETL y Delta Lake",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "ceste",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
