{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83c5257b-dc44-47af-82b5-c67e4798783d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Integración Final: Modelado en Capas, Modelado de Datos y Práctica Completa\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f0aebcc-a2c0-4504-9c41-7b1556bb4b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "En esta sesión integraremos todo lo aprendido durante el curso:\n",
    " - Arquitectura de datos en capas (Bronze, Silver, Gold)\n",
    " - Modelado de datos en Databricks con Delta Lake\n",
    " - Automatización y orquestación con Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "944ff316-cd14-4a68-93a3-8508caa9204c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parte 1: Arquitectura en Capas\n",
    "\n",
    "En Databricks, el **modelo en capas** es una práctica recomendada para estructurar los datos:\n",
    "\n",
    "### 1. **Bronze** (Capa cruda)\n",
    "- Datos tal cual vienen del origen.\n",
    "- Poco o ningún procesamiento.\n",
    "- Se usa para trazabilidad y auditoría.\n",
    "\n",
    "### 2. **Silver** (Capa limpia)\n",
    "- Datos depurados, con tipos bien definidos.\n",
    "- Se aplican validaciones y joins si es necesario.\n",
    "- Se preparan para análisis y agregaciones.\n",
    "\n",
    "### 3. **Gold** (Capa de negocio)\n",
    "- Métricas clave, KPIs.\n",
    "- Datos listos para dashboards o consumo analítico.\n",
    "\n",
    "Cada capa se almacena en tablas Delta y puede tener gobernanza a través de Unity Catalog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8287d43-ecbc-4519-92b9-e445c5793793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parte 2: Modelado de Datos\n",
    "\n",
    "Para este caso, usamos una variante del **modelo en estrella (Star Schema)**:\n",
    "\n",
    "- **Tabla de hechos**: vuelos y mantenimientos\n",
    "- **Tablas de dimensión**: aeropuertos, aeronaves\n",
    "\n",
    "Este modelo nos permite hacer análisis por rutas, retrasos por aeropuerto, rendimiento por aerolínea, etc.\n",
    "\n",
    "Ejemplo de dimensiones:\n",
    "- `dim_aeropuertos (aeropuerto_id, nombre, ciudad, pais, lat, lon)`\n",
    "- `dim_aeronaves (aeronave_id, modelo, fabricante, anio_fabricacion)`\n",
    "\n",
    "Ejemplo de hecho:\n",
    "- `fact_vuelos (vuelo_id, fecha, origen_id, destino_id, aeronave_id, estado, duracion_min)`\n",
    "- `fact_mantenimientos (mantenimiento_id, aeronave_id, fecha, tipo, costo_usd, duracion_hr)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "048c6bdc-2163-445d-9b81-bedfd82e8c7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parte 3: Práctica Final\n",
    "\n",
    "### Objetivo\n",
    "Crear un pipeline de datos (Workflow) partiendo de los ficheros csv que tienen que ser ingestado en `bronce`, hacer una serie de validaciones para cargar estos datos posteriormente en `plata`, y por último crear 2 procesos independientes para generar 2 tablas en `oro`: una calculando un KPI de puntualidad por mes y modelo de aeronave y otro proceso para calcular osto promedio de mantenimiento por tipo y modelo de aeronave\n",
    "\n",
    "### Archivos utilizados\n",
    "- `aerolineas.csv`\n",
    "- `aeropuertos.csv`\n",
    "- `vuelos.csv`\n",
    "- `mantenimientos.csv`\n",
    "\n",
    "### Pasos:\n",
    "0. Crear un workflow\n",
    "1. Ingesta de datos crudos a capa **bronce**. (*Nota en la siguiente celda*):\n",
    "    - Cargar los datos en el `Catalog`\n",
    "    - Crear un script `.py` que sea parametrizable para cargar los datos desde los ficheros a tablas delta.\n",
    "    - Este script tiene que tener 4 parametros: `input_path`, `output_path`, `write_mode`, `partition_by`\n",
    "    - Crear tareas para ingestar los archivos utilizando los puntos anteriores\n",
    "2. Limpieza y relaciones en **plata**\n",
    "    - Revisar el fichero `config.json`\n",
    "    - Crear un script de validación de datos que lea de una tabla delta, que compruebe los puntos del fichero anterior y escriba en la capa `plata`. \n",
    "    - Este script tiene que ser parametrizable con los siguientes argumentos: `input_table`, `output_table`, `config_path`, `table_name`\n",
    "3. Generación de métricas en **oro**\n",
    "    - Crear dos script uno para cada proceso\n",
    "    - Analizar los datos de los ficheros y comprobar cómo podría llevarse acabo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cce4122d-a2dc-438a-b017-af2964f1ae0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "*Nota*: La idea original era utilizar una fuente de datos externa e ingestarla. Esto se podía realizar de dos formas:\n",
    " - Mediante un ingestion pipeline pero daba error por permisos\n",
    " - Mediante un script consultando una fuente de datos pero el cluster no tiene salida a internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e038c083-2b91-450e-b415-39435c038019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get(\"https://databricks-ceste.s3.eu-west-1.amazonaws.com/mantenimientos_rds.csv\")\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b5038b4-cb27-480c-a91a-a05c728db8da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Puntos fuertes de este proceso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae5bb2d7-eb7d-4034-940f-a6c0d517b960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Ingesta de múltiples orígenes\n",
    "2. El modelado de capas facilita el debug, permite reusabilidad y versionamiento de datos intermedios y escalar a nuevos pipeline sin duplicar código\n",
    "3. La orquestación con workflow permite automatización "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79129536-20aa-4497-9a2c-8b01e70af247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pasos Opcionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a70fe9b-c307-45ec-bc3a-61b7c16bf7bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Ingestar otro fichero para las tablas de hechos y para las de dimensiones y ver la diferencia (append /overwrite)\n",
    "2. Automatizar nueva llegada de ficheros\n",
    "3. Diseño de logs\n",
    "4. Creacion de permisos con Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f795514f-e1ff-461d-8190-ee12bea5c556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3. Databricks en entornos productivos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
